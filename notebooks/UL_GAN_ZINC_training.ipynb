{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_folder = '../codes'\n",
    "import sys\n",
    "sys.path.append(code_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import packages\n",
    "import numpy as np\n",
    "from rdkit import RDLogger    \n",
    "from torch_geometric.data import Data\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "from util_gnn import draw_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miniconda3/envs/gnn/lib/python3.9/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "### Load data\n",
    "batch_size = 64\n",
    "mol_data = torch.load('zinc_smiles_noar.pt')\n",
    "data_list = torch.load('zinc_data_noar.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just use 11-36 nodes graphs for training.\n",
    "data_list = [Data(x=j.x, edge_index=j.edge_index, edge_attr=j.edge_attr, y = j.y) \\\n",
    "             for i, j in enumerate(data_list) if len(j.x) >= 11 and len(j.x) <= 36]\n",
    "data_loader = DataLoader(data_list, batch_size=batch_size, shuffle=True, follow_batch=['edge_index', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9982842665640433"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)/249456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn_model_sim_summ_z2 import *\n",
    "from ugcn_model_summ_2 import pre_GCNModel_edge_3eos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\" if(torch.cuda.is_available()) else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build discriminator\n",
    "gcn_model = pre_GCNModel_edge_3eosZv4(in_dim=15, \\\n",
    "                                   hidden_dim=128, \\\n",
    "                                   edge_dim=3, \\\n",
    "                                   edge_hidden_dim=32, \\\n",
    "                                   lin_hidden_dim=128, \\\n",
    "                                   out_hidden_dim=256, \\\n",
    "                                   device=device, \\\n",
    "                                    check_batch=None, \n",
    "                                    useBN=True, \\\n",
    "                                    droprate=0.3, \n",
    "                                    pool_method='trivial', \\\n",
    "                                    add_edge_link=False, \n",
    "                                    add_conv_link=True, \\\n",
    "                                    outBN=True, out_drop=0.3, \n",
    "                                    out_divide=4.0, \n",
    "                                    add_edge_agg=False, \n",
    "                                    real_trivial=False, \n",
    "                                    final_layers=2, \n",
    "                                    add_trivial_feature=False, ln_comp=False, without_ar=True).to(device)\n",
    "\n",
    "# This is a trivial discriminator...\n",
    "d_add = pre_GCNModel_edge_3eosZ(in_dim=15, \\\n",
    "                                   hidden_dim=64, \\\n",
    "                                   edge_dim=3, \\\n",
    "                                   edge_hidden_dim=32, \\\n",
    "                                   lin_hidden_dim=64, \\\n",
    "                                   out_hidden_dim=128, \\\n",
    "                                   device=device, \\\n",
    "                                    check_batch=None, \n",
    "                                    useBN=False, \\\n",
    "                                    droprate=0.3, \n",
    "                                    pool_method='trivial', \\\n",
    "                                    add_edge_link=False, \n",
    "                                    add_conv_link=False, \\\n",
    "                                    outBN=False, out_drop=0.3, \n",
    "                                    out_divide=4.0, \n",
    "                                    add_edge_agg=False, \n",
    "                                    real_trivial=True, \n",
    "                                    final_layers=2, \n",
    "                                    add_trivial_feature=True, ln_comp=False, without_ar=True).to(device)\n",
    "d_add.e1_ind = 0.0 # no means, just sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build generator\n",
    "generator = UnpoolGeneratorZ(in_dim=128, \\\n",
    "                            edge_dim=3, \n",
    "                            node_dim=15, \n",
    "                            node_hidden_dim=[32, 32, 64, 64, 64, 128, 128, 128, 256], \n",
    "                            edge_hidden_dim=32, \\\n",
    "                            use_x_bn=True, \n",
    "                            use_e_bn=True, \n",
    "                            unpool_bn=True, \n",
    "                            link_bn=True, \n",
    "                            attr_bn=True, \n",
    "                            skip_z=True, \n",
    "                            skip_zdim=None, \n",
    "                            conv_type='nn', \n",
    "                            device=device, \n",
    "                            last_act='leaky', \n",
    "                            link_act='leaky', \\\n",
    "                            unpool_type='edge', unpool_para=dict(add_perference=True, roll_bn=False, roll_simple=True), \n",
    "                             without_ar=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import GANTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = GANTrainer(d=gcn_model, g=generator, \\\n",
    "                   rand_dim=128, train_folder='ULGAN_ZINC', \\\n",
    "                   tot_epoch_num=100, eval_iter_num=1000, \\\n",
    "                   batch_size=64, \\\n",
    "                   device=device, d_add=d_add, \\\n",
    "                   learning_rate_g=1e-3, learning_rate_d=2e-4, \\\n",
    "                   lambda_g=10.0, \\\n",
    "                   max_train_G=2, \\\n",
    "                   tresh_add_trainG=0.2, \\\n",
    "                   use_loss='wgan', \\\n",
    "                   g_out_prob=True, \\\n",
    "                   lambda_rl=2e-3, lambda_nonodes = 0., \n",
    "                   lambda_noedges = 0., zinc=True, without_ar=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200][0/3892]\tG Loss: 0.5876;D Loss: -1.3593; GP: 0.2405\n",
      "now, we train G 2 times with (prob fake = -0.588, prob real = 0.775)\n",
      "Mean x/edge attr:  tensor([0.7179, 0.0983, 0.1393, 0.0273, 0.0172, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.9828, 0.0172, 0.0000, 0.9777, 0.0036, 0.0187], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7482, 0.2518, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(43.8750, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 21.765625 tensor([25., 23., 24., 24., 24., 21., 27., 16., 22., 24., 23., 26., 26., 23.,\n",
      "        21., 24., 17., 23., 23., 23., 24., 20., 28., 14., 23., 23., 26., 28.,\n",
      "        20., 19., 23., 22., 22., 18., 23., 24., 20., 25., 20., 26., 24., 15.,\n",
      "        16., 23., 21., 19., 20., 24., 23., 20., 24., 20., 15., 20., 18., 23.,\n",
      "        20., 17., 16., 13., 25., 21., 22., 27.], device='cuda:2')\n",
      "Sample prob: -24.980266571044922\n",
      "Validation, uniqueness, novelty:  (0.625, 1.0, 1.0)\n",
      "[1/200][100/3892]\tG Loss: 0.3311;D Loss: -1.3127; GP: 1.0413\n",
      "now, we train G 2 times with (prob fake = -0.331, prob real = 1.007)\n",
      "Mean x/edge attr:  tensor([7.4458e-01, 1.3526e-01, 1.1097e-01, 6.5660e-04, 8.5358e-03, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 9.7768e-01, 2.2324e-02, 0.0000e+00,\n",
      "        9.3762e-01, 1.1162e-02, 5.1215e-02], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7449, 0.2551, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(51.6250, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.796875 tensor([14., 26., 25., 29., 25., 27., 24., 27., 27., 25., 26., 21., 27., 28.,\n",
      "        16., 27., 22., 16., 26., 27., 25., 11., 29., 27., 25., 23., 26., 26.,\n",
      "        28., 23., 19., 25., 25., 23., 26., 27., 27., 27., 27., 25., 30., 28.,\n",
      "        23., 27., 13., 24., 26., 25., 12., 23., 27., 12., 22., 23., 12., 26.,\n",
      "        25., 17., 28., 15., 27., 29., 26., 24.], device='cuda:2')\n",
      "Sample prob: -30.368091583251953\n",
      "[1/200][200/3892]\tG Loss: 0.4583;D Loss: -1.1916; GP: 0.3203\n",
      "now, we train G 2 times with (prob fake = -0.458, prob real = 0.748)\n",
      "Mean x/edge attr:  tensor([0.7275, 0.1114, 0.0838, 0.0018, 0.0509, 0.0018, 0.0228, 0.0000, 0.0000,\n",
      "        0.9826, 0.0174, 0.0000, 0.9635, 0.0192, 0.0174], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7028, 0.2972, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(55.4688, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 26.09375 tensor([30., 17., 32., 27., 23., 26., 16., 28., 29., 21., 26., 21., 30., 29.,\n",
      "        26., 30., 29., 24., 15., 30., 31., 27., 33., 14., 25., 28., 32., 27.,\n",
      "        29., 23., 25., 28., 24., 28., 30., 31., 27., 31., 15., 31., 28., 20.,\n",
      "        24., 18., 29., 29., 22., 31., 19., 29., 29., 28., 26., 23., 27., 30.,\n",
      "        28., 28., 21., 29., 25., 27., 21., 31.], device='cuda:2')\n",
      "Sample prob: -30.54903793334961\n",
      "Validation, uniqueness, novelty:  (0.19, 1.0, 1.0)\n",
      "[1/200][300/3892]\tG Loss: 0.8421;D Loss: -1.4132; GP: 0.3011\n",
      "now, we train G 2 times with (prob fake = -0.842, prob real = 0.553)\n",
      "Mean x/edge attr:  tensor([7.6440e-01, 1.1152e-01, 9.0103e-02, 7.3855e-04, 8.1241e-03, 2.5111e-02,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 9.6307e-01, 3.6928e-02, 0.0000e+00,\n",
      "        9.5052e-01, 1.8464e-02, 3.1019e-02], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.8050, 0.1950, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(43.5938, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 21.15625 tensor([13., 28., 28., 19., 24., 18., 21., 21., 25., 19., 16., 21., 15., 23.,\n",
      "        14., 21., 25., 25., 20., 30., 20., 24., 18., 26., 19., 22., 22., 14.,\n",
      "        23., 29., 17., 22., 18., 27., 19., 21., 16., 23., 31., 19., 19., 18.,\n",
      "        24., 20., 23., 20., 22., 29., 19., 23., 22., 13., 15., 22., 24., 24.,\n",
      "        12., 26., 18., 25., 22., 21., 16., 21.], device='cuda:2')\n",
      "Sample prob: -26.164810180664062\n",
      "[1/200][400/3892]\tG Loss: 0.5827;D Loss: -1.5434; GP: 0.4832\n",
      "now, we train G 2 times with (prob fake = -0.583, prob real = 1.004)\n",
      "Mean x/edge attr:  tensor([7.1943e-01, 1.4254e-01, 8.1770e-02, 9.7524e-03, 3.3758e-02, 1.2753e-02,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9850e-01, 1.5004e-03, 0.0000e+00,\n",
      "        9.9025e-01, 7.5019e-04, 9.0023e-03], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7744, 0.2256, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(43.8438, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 20.828125 tensor([28., 14., 12., 17., 31., 23., 30., 33., 18., 15., 21., 21., 19., 27.,\n",
      "        24., 22., 12., 12., 14., 22., 28., 24., 12., 22., 17., 19., 20., 12.,\n",
      "        26., 29., 29., 16., 23., 22., 21., 21., 18., 17., 29., 24., 11., 23.,\n",
      "        17., 23., 21., 21., 25., 12., 13., 24., 11., 26., 27., 29., 23., 21.,\n",
      "        17., 21., 26., 22., 15., 17., 30., 14.], device='cuda:2')\n",
      "Sample prob: -25.078617095947266\n",
      "Validation, uniqueness, novelty:  (0.262, 1.0, 1.0)\n",
      "[1/200][500/3892]\tG Loss: 0.5212;D Loss: -1.3420; GP: 0.5679\n",
      "now, we train G 2 times with (prob fake = -0.521, prob real = 0.816)\n",
      "Mean x/edge attr:  tensor([0.7244, 0.1311, 0.1146, 0.0116, 0.0183, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.9896, 0.0091, 0.0012, 0.9567, 0.0287, 0.0146], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7198, 0.2802, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(56.7188, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 25.625 tensor([28., 21., 30., 24., 31., 27., 27., 33., 32., 17., 22., 29., 25., 28.,\n",
      "        19., 26., 31., 29., 22., 14., 31., 33., 22., 22., 23., 16., 17., 28.,\n",
      "        12., 15., 24., 31., 14., 33., 17., 30., 32., 33., 29., 26., 32., 16.,\n",
      "        24., 31., 32., 28., 17., 20., 20., 26., 33., 32., 28., 12., 19., 33.,\n",
      "        31., 28., 32., 33., 33., 20., 30., 27.], device='cuda:2')\n",
      "Sample prob: -31.204681396484375\n",
      "[1/200][600/3892]\tG Loss: 0.7956;D Loss: -1.2732; GP: 0.4196\n",
      "now, we train G 2 times with (prob fake = -0.796, prob real = 0.467)\n",
      "Mean x/edge attr:  tensor([0.7514, 0.1157, 0.1108, 0.0028, 0.0132, 0.0062, 0.0000, 0.0000, 0.0000,\n",
      "        0.9501, 0.0499, 0.0000, 0.9584, 0.0277, 0.0139], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([7.3721e-01, 2.6247e-01, 3.1969e-04], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor(48.8750, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 22.5625 tensor([23., 18., 17., 21., 16., 24., 22., 24., 18., 24., 24., 19., 22., 25.,\n",
      "        28., 25., 18., 25., 22., 14., 24., 21., 28., 14., 18., 29., 24., 21.,\n",
      "        14., 21., 22., 18., 24., 24., 23., 20., 20., 18., 28., 20., 20., 24.,\n",
      "        29., 25., 21., 23., 33., 27., 24., 22., 26., 24., 32., 24., 22., 22.,\n",
      "        31., 19., 26., 15., 29., 21., 23., 22.], device='cuda:2')\n",
      "Sample prob: -25.528343200683594\n",
      "Validation, uniqueness, novelty:  (0.228, 1.0, 1.0)\n",
      "[1/200][700/3892]\tG Loss: 0.7318;D Loss: -1.4154; GP: 0.4333\n",
      "now, we train G 2 times with (prob fake = -0.732, prob real = 0.701)\n",
      "Mean x/edge attr:  tensor([0.7226, 0.1111, 0.1251, 0.0180, 0.0126, 0.0106, 0.0000, 0.0000, 0.0000,\n",
      "        0.9953, 0.0047, 0.0000, 0.9661, 0.0233, 0.0106], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7181, 0.2819, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(51.7188, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.484375 tensor([26., 14., 24., 28., 28., 29., 27., 19., 30., 14., 19., 13., 21., 25.,\n",
      "        14., 32., 22., 19., 18., 30., 23., 26., 23., 24., 18., 22., 26., 20.,\n",
      "        25., 13., 26., 27., 21., 27., 26., 28., 21., 20., 30., 22., 25., 33.,\n",
      "        26., 19., 18., 24., 24., 26., 28., 24., 22., 24., 29., 23., 17., 13.,\n",
      "        25., 32., 24., 25., 29., 30., 18., 25.], device='cuda:2')\n",
      "Sample prob: -28.48088836669922\n",
      "[1/200][800/3892]\tG Loss: 0.7864;D Loss: -1.4040; GP: 0.4233\n",
      "now, we train G 2 times with (prob fake = -0.786, prob real = 0.619)\n",
      "Mean x/edge attr:  tensor([7.2698e-01, 1.0818e-01, 1.2556e-01, 1.8674e-02, 1.9961e-02, 0.0000e+00,\n",
      "        6.4392e-04, 0.0000e+00, 0.0000e+00, 9.8133e-01, 1.6098e-02, 2.5757e-03,\n",
      "        9.6974e-01, 1.0947e-02, 1.9317e-02], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7158, 0.2842, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(51.5625, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 24.265625 tensor([28., 33., 31., 21., 21., 21., 17., 18., 26., 22., 23., 26., 30., 18.,\n",
      "        27., 33., 17., 22., 23., 31., 30., 18., 17., 27., 22., 17., 28., 24.,\n",
      "        32., 33., 18., 16., 33., 31., 19., 30., 15., 30., 28., 21., 22., 29.,\n",
      "        24., 30., 19., 23., 30., 24., 26., 22., 16., 29., 31., 20., 20., 18.,\n",
      "        24., 25., 31., 23., 15., 31., 17., 27.], device='cuda:2')\n",
      "Sample prob: -28.164770126342773\n",
      "Validation, uniqueness, novelty:  (0.221, 1.0, 1.0)\n",
      "[1/200][900/3892]\tG Loss: 0.7903;D Loss: -1.4253; GP: 0.2761\n",
      "now, we train G 2 times with (prob fake = -0.790, prob real = 0.665)\n",
      "Mean x/edge attr:  tensor([0.7069, 0.1441, 0.1143, 0.0194, 0.0152, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.9841, 0.0104, 0.0055, 0.9792, 0.0035, 0.0173], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7419, 0.2581, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(48.3125, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 22.546875 tensor([26., 18., 29., 31., 25., 33., 25., 19., 23., 22., 17., 21., 32., 21.,\n",
      "        25., 19., 18., 19., 21., 16., 19., 32., 27., 26., 33., 17., 21., 26.,\n",
      "        22., 20., 18., 18., 23., 25., 18., 22., 24., 16., 14., 32., 19., 29.,\n",
      "        18., 27., 19., 19., 26., 21., 33., 16., 20., 20., 21., 22., 18., 16.,\n",
      "        20., 26., 24., 22., 20., 26., 19., 29.], device='cuda:2')\n",
      "Sample prob: -26.485937118530273\n",
      "[1/200][1000/3892]\tG Loss: 0.8262;D Loss: -1.4638; GP: 0.3270\n",
      "now, we train G 2 times with (prob fake = -0.826, prob real = 0.654)\n",
      "Mean x/edge attr:  tensor([7.5294e-01, 1.0809e-01, 1.0588e-01, 2.2794e-02, 5.8824e-03, 7.3529e-04,\n",
      "        3.6765e-03, 0.0000e+00, 0.0000e+00, 9.8603e-01, 1.3971e-02, 0.0000e+00,\n",
      "        9.2721e-01, 3.6765e-03, 6.9118e-02], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7757, 0.2208, 0.0035], device='cuda:2', grad_fn=<MeanBackward1>) tensor(45.2188, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 21.25 tensor([20., 15., 21., 19., 18., 28., 24., 20., 21., 27., 14., 25., 22., 21.,\n",
      "        19., 23., 14., 23., 32., 20., 23., 22., 17., 23., 17., 25., 20., 24.,\n",
      "        21., 19., 23., 22., 23., 19., 12., 24., 19., 25., 24., 17., 24., 24.,\n",
      "        21., 23., 23., 20., 25., 24., 19., 26., 19., 17., 19., 20., 18., 17.,\n",
      "        18., 22., 22., 19., 19., 23., 31., 22.], device='cuda:2')\n",
      "Sample prob: -25.43506622314453\n",
      "Validation, uniqueness, novelty:  (0.379, 1.0, 1.0)\n",
      "[1/200][1100/3892]\tG Loss: 0.7193;D Loss: -1.4157; GP: 0.4681\n",
      "now, we train G 2 times with (prob fake = -0.719, prob real = 0.684)\n",
      "Mean x/edge attr:  tensor([0.7059, 0.1246, 0.0955, 0.0152, 0.0240, 0.0348, 0.0000, 0.0000, 0.0000,\n",
      "        0.9949, 0.0051, 0.0000, 0.9412, 0.0487, 0.0101], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7027, 0.2973, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(53.6562, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 24.703125 tensor([25., 17., 18., 23., 31., 26., 20., 22., 28., 31., 20., 19., 27., 21.,\n",
      "        21., 19., 18., 32., 24., 21., 30., 23., 23., 18., 19., 23., 19., 32.,\n",
      "        27., 19., 33., 29., 33., 22., 18., 32., 32., 20., 21., 33., 24., 31.,\n",
      "        30., 32., 29., 29., 26., 16., 23., 23., 23., 32., 33., 20., 25., 20.,\n",
      "        30., 17., 19., 23., 22., 30., 33., 22.], device='cuda:2')\n",
      "Sample prob: -29.317115783691406\n",
      "[1/200][1200/3892]\tG Loss: 0.8404;D Loss: -1.5852; GP: 0.4396\n",
      "now, we train G 2 times with (prob fake = -0.840, prob real = 0.718)\n",
      "Mean x/edge attr:  tensor([0.7445, 0.1202, 0.1217, 0.0000, 0.0045, 0.0076, 0.0015, 0.0000, 0.0000,\n",
      "        0.9917, 0.0083, 0.0000, 0.9017, 0.0862, 0.0121], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7658, 0.2217, 0.0125], device='cuda:2', grad_fn=<MeanBackward1>) tensor(43.9062, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 20.671875 tensor([22., 14., 20., 20., 23., 18., 24., 23., 14., 18., 18., 22., 24., 21.,\n",
      "        25., 23., 24., 22., 19., 24., 15., 22., 21., 23., 16., 28., 24., 18.,\n",
      "        16., 24., 20., 19., 18., 25., 18., 18., 22., 13., 25., 23., 19., 24.,\n",
      "        23., 18., 22., 20., 18., 15., 19., 26., 20., 19., 27., 26., 20., 22.,\n",
      "        16., 16., 29., 22., 16., 19., 20., 21.], device='cuda:2')\n",
      "Sample prob: -26.058494567871094\n",
      "Validation, uniqueness, novelty:  (0.332, 1.0, 1.0)\n",
      "[1/200][1300/3892]\tG Loss: 0.7045;D Loss: -1.3999; GP: 0.4019\n",
      "now, we train G 2 times with (prob fake = -0.705, prob real = 0.766)\n",
      "Mean x/edge attr:  tensor([0.7497, 0.1170, 0.1099, 0.0020, 0.0065, 0.0124, 0.0026, 0.0000, 0.0000,\n",
      "        0.9980, 0.0020, 0.0000, 0.9616, 0.0072, 0.0312], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7126, 0.2728, 0.0146], device='cuda:2', grad_fn=<MeanBackward1>) tensor(54.4688, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 24.03125 tensor([19., 22., 25., 23., 24., 24., 23., 27., 24., 22., 23., 21., 27., 25.,\n",
      "        24., 26., 28., 28., 31., 27., 24., 23., 23., 29., 28., 22., 24., 21.,\n",
      "        26., 18., 25., 21., 23., 28., 21., 24., 26., 23., 29., 24., 20., 24.,\n",
      "        20., 26., 24., 17., 26., 25., 24., 23., 31., 28., 25., 20., 22., 21.,\n",
      "        21., 21., 18., 30., 22., 22., 26., 27.], device='cuda:2')\n",
      "Sample prob: -30.0008487701416\n",
      "[1/200][1400/3892]\tG Loss: 0.8059;D Loss: -1.4143; GP: 0.3453\n",
      "now, we train G 2 times with (prob fake = -0.806, prob real = 0.604)\n",
      "Mean x/edge attr:  tensor([7.3500e-01, 1.3891e-01, 8.2940e-02, 2.2252e-02, 2.0229e-02, 6.7431e-04,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 9.5887e-01, 4.1133e-02, 0.0000e+00,\n",
      "        9.6696e-01, 6.7431e-04, 3.2367e-02], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7426, 0.2559, 0.0016], device='cuda:2', grad_fn=<MeanBackward1>) tensor(49.4062, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.171875 tensor([25., 20., 21., 27., 25., 21., 22., 18., 26., 25., 26., 25., 24., 17.,\n",
      "        23., 24., 25., 25., 25., 20., 21., 20., 20., 21., 20., 25., 25., 20.,\n",
      "        20., 25., 26., 28., 24., 27., 19., 23., 23., 25., 19., 19., 22., 20.,\n",
      "        31., 21., 22., 23., 20., 27., 28., 29., 26., 20., 24., 21., 20., 23.,\n",
      "        24., 23., 21., 21., 26., 26., 27., 24.], device='cuda:2')\n",
      "Sample prob: -29.62796401977539\n",
      "Validation, uniqueness, novelty:  (0.357, 1.0, 1.0)\n",
      "[1/200][1500/3892]\tG Loss: 0.7676;D Loss: -1.4369; GP: 0.2175\n",
      "now, we train G 2 times with (prob fake = -0.768, prob real = 0.692)\n",
      "Mean x/edge attr:  tensor([7.2588e-01, 1.1555e-01, 1.0203e-01, 9.8341e-03, 1.7210e-02, 2.8888e-02,\n",
      "        6.1463e-04, 0.0000e+00, 0.0000e+00, 9.5759e-01, 6.1463e-03, 3.6263e-02,\n",
      "        9.6620e-01, 1.4136e-02, 1.9668e-02], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7029, 0.2925, 0.0046], device='cuda:2', grad_fn=<MeanBackward1>) tensor(54.2812, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 25.421875 tensor([33., 28., 27., 19., 18., 22., 28., 27., 27., 18., 32., 28., 29., 25.,\n",
      "        24., 24., 25., 23., 25., 24., 22., 22., 31., 28., 29., 25., 23., 31.,\n",
      "        29., 27., 24., 17., 22., 29., 31., 23., 31., 21., 32., 19., 26., 22.,\n",
      "        28., 31., 23., 31., 23., 30., 23., 20., 23., 33., 23., 21., 21., 29.,\n",
      "        29., 21., 25., 23., 26., 27., 29., 18.], device='cuda:2')\n",
      "Sample prob: -31.746875762939453\n",
      "[1/200][1600/3892]\tG Loss: 0.6713;D Loss: -1.4955; GP: 0.3802\n",
      "now, we train G 2 times with (prob fake = -0.671, prob real = 0.798)\n",
      "Mean x/edge attr:  tensor([7.4919e-01, 1.0489e-01, 9.4463e-02, 1.1075e-02, 2.0195e-02, 3.2573e-03,\n",
      "        1.6938e-02, 0.0000e+00, 0.0000e+00, 9.9935e-01, 6.5147e-04, 0.0000e+00,\n",
      "        9.6743e-01, 1.0423e-02, 2.2150e-02], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7187, 0.2743, 0.0070], device='cuda:2', grad_fn=<MeanBackward1>) tensor(51.4375, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.984375 tensor([23., 25., 18., 21., 26., 21., 24., 21., 29., 29., 29., 27., 27., 19.,\n",
      "        28., 27., 22., 29., 22., 22., 19., 21., 17., 31., 25., 31., 20., 18.,\n",
      "        25., 18., 20., 29., 20., 22., 22., 31., 19., 19., 29., 23., 24., 26.,\n",
      "        31., 24., 27., 30., 31., 23., 22., 18., 22., 18., 29., 20., 29., 26.,\n",
      "        23., 24., 28., 22., 27., 29., 15., 19.], device='cuda:2')\n",
      "Sample prob: -30.894332885742188\n",
      "Validation, uniqueness, novelty:  (0.319, 1.0, 1.0)\n",
      "[1/200][1700/3892]\tG Loss: 0.9072;D Loss: -1.5600; GP: 0.4765\n",
      "now, we train G 2 times with (prob fake = -0.907, prob real = 0.674)\n",
      "Mean x/edge attr:  tensor([0.7241, 0.1309, 0.1098, 0.0077, 0.0106, 0.0113, 0.0056, 0.0000, 0.0000,\n",
      "        0.9894, 0.0106, 0.0000, 0.9796, 0.0056, 0.0148], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7060, 0.2871, 0.0069], device='cuda:2', grad_fn=<MeanBackward1>) tensor(49.5312, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 22.203125 tensor([20., 19., 22., 25., 19., 25., 16., 25., 17., 26., 23., 22., 20., 17.,\n",
      "        27., 19., 22., 27., 27., 19., 19., 20., 21., 20., 24., 23., 23., 20.,\n",
      "        20., 25., 19., 25., 28., 23., 19., 21., 20., 25., 25., 21., 21., 23.,\n",
      "        16., 31., 27., 22., 21., 18., 18., 22., 29., 31., 20., 29., 22., 16.,\n",
      "        19., 22., 25., 21., 23., 27., 20., 20.], device='cuda:2')\n",
      "Sample prob: -27.865272521972656\n",
      "[1/200][1800/3892]\tG Loss: 0.7860;D Loss: -1.4786; GP: 0.4643\n",
      "now, we train G 2 times with (prob fake = -0.786, prob real = 0.672)\n",
      "Mean x/edge attr:  tensor([0.7382, 0.1292, 0.0875, 0.0048, 0.0171, 0.0164, 0.0068, 0.0000, 0.0000,\n",
      "        0.9672, 0.0048, 0.0280, 0.9733, 0.0219, 0.0048], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7380, 0.2607, 0.0013], device='cuda:2', grad_fn=<MeanBackward1>) tensor(47.5938, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 22.859375 tensor([19., 18., 16., 27., 18., 17., 29., 20., 21., 23., 27., 23., 30., 17.,\n",
      "        29., 21., 29., 17., 23., 27., 26., 13., 29., 28., 21., 27., 25., 17.,\n",
      "        23., 27., 26., 21., 25., 29., 20., 31., 30., 25., 18., 14., 23., 17.,\n",
      "        20., 25., 29., 15., 25., 14., 24., 21., 29., 27., 20., 23., 23., 15.,\n",
      "        27., 18., 29., 28., 24., 22., 22., 17.], device='cuda:2')\n",
      "Sample prob: -29.769458770751953\n",
      "Validation, uniqueness, novelty:  (0.226, 1.0, 1.0)\n",
      "[1/200][1900/3892]\tG Loss: 0.9509;D Loss: -1.5470; GP: 0.4807\n",
      "now, we train G 2 times with (prob fake = -0.951, prob real = 0.532)\n",
      "Mean x/edge attr:  tensor([7.3995e-01, 1.2737e-01, 9.4769e-02, 1.6679e-02, 1.6679e-02, 3.7908e-03,\n",
      "        7.5815e-04, 0.0000e+00, 0.0000e+00, 9.7650e-01, 2.2745e-02, 7.5815e-04,\n",
      "        9.4466e-01, 3.2600e-02, 2.2745e-02], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7595, 0.2405, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(43.4688, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 20.609375 tensor([25., 23., 21., 25., 20., 19., 15., 21., 15., 14., 23., 16., 23., 13.,\n",
      "        23., 21., 21., 20., 15., 25., 26., 19., 23., 25., 18., 25., 27., 20.,\n",
      "        19., 18., 25., 23., 20., 23., 23., 20., 22., 23., 23., 23., 19., 25.,\n",
      "        19., 18., 21., 20., 20., 14., 21., 21., 31., 19., 18., 15., 17., 17.,\n",
      "        25., 20., 20., 19., 25., 21., 16., 15.], device='cuda:2')\n",
      "Sample prob: -25.212566375732422\n",
      "[1/200][2000/3892]\tG Loss: 0.8874;D Loss: -1.5591; GP: 0.2141\n",
      "now, we train G 2 times with (prob fake = -0.887, prob real = 0.660)\n",
      "Mean x/edge attr:  tensor([0.7487, 0.1314, 0.0831, 0.0040, 0.0154, 0.0141, 0.0034, 0.0000, 0.0000,\n",
      "        0.9718, 0.0282, 0.0000, 0.9839, 0.0074, 0.0087], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7313, 0.2687, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(51.0625, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.3125 tensor([31., 31., 19., 25., 31., 25., 20., 25., 19., 20., 23., 23., 16., 26.,\n",
      "        22., 23., 20., 17., 25., 24., 26., 20., 31., 20., 19., 18., 20., 21.,\n",
      "        26., 20., 23., 20., 23., 23., 20., 26., 25., 29., 27., 25., 25., 27.,\n",
      "        18., 17., 29., 27., 29., 18., 24., 21., 29., 25., 20., 23., 21., 18.,\n",
      "        27., 26., 22., 17., 33., 19., 19., 31.], device='cuda:2')\n",
      "Sample prob: -29.492088317871094\n",
      "Validation, uniqueness, novelty:  (0.271, 1.0, 1.0)\n",
      "[1/200][2100/3892]\tG Loss: 0.9202;D Loss: -1.3801; GP: 0.4240\n",
      "now, we train G 2 times with (prob fake = -0.920, prob real = 0.538)\n",
      "Mean x/edge attr:  tensor([7.2771e-01, 1.2526e-01, 9.4622e-02, 1.9741e-02, 2.3145e-02, 9.5303e-03,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 9.7617e-01, 2.3145e-02, 6.8074e-04,\n",
      "        9.6869e-01, 2.1784e-02, 9.5303e-03], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7643, 0.2287, 0.0071], device='cuda:2', grad_fn=<MeanBackward1>) tensor(48.7188, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 22.953125 tensor([30., 25., 22., 22., 20., 29., 33., 33., 28., 23., 16., 17., 26., 25.,\n",
      "        18., 22., 27., 25., 27., 18., 21., 20., 15., 27., 21., 29., 18., 26.,\n",
      "        22., 24., 17., 18., 18., 24., 25., 27., 18., 24., 17., 31., 17., 29.,\n",
      "        18., 17., 24., 27., 28., 20., 20., 29., 32., 15., 16., 20., 17., 13.,\n",
      "        31., 17., 18., 27., 27., 31., 23., 25.], device='cuda:2')\n",
      "Sample prob: -26.509490966796875\n",
      "[1/200][2200/3892]\tG Loss: 0.7978;D Loss: -1.3642; GP: 0.5983\n",
      "now, we train G 2 times with (prob fake = -0.798, prob real = 0.593)\n",
      "Mean x/edge attr:  tensor([0.7194, 0.1344, 0.0925, 0.0125, 0.0175, 0.0088, 0.0150, 0.0000, 0.0000,\n",
      "        0.9669, 0.0319, 0.0012, 0.9656, 0.0106, 0.0237], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7377, 0.2553, 0.0069], device='cuda:2', grad_fn=<MeanBackward1>) tensor(54.1562, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 25.0 tensor([15., 33., 19., 27., 21., 33., 30., 28., 31., 31., 33., 33., 29., 17.,\n",
      "        19., 26., 33., 19., 24., 19., 18., 20., 21., 29., 24., 20., 21., 23.,\n",
      "        17., 26., 33., 19., 31., 24., 22., 20., 33., 31., 18., 16., 33., 26.,\n",
      "        17., 23., 29., 21., 23., 25., 31., 19., 20., 28., 26., 21., 19., 31.,\n",
      "        31., 33., 26., 31., 20., 30., 31., 20.], device='cuda:2')\n",
      "Sample prob: -27.77895164489746\n",
      "Validation, uniqueness, novelty:  (0.182, 1.0, 1.0)\n",
      "[1/200][2300/3892]\tG Loss: 0.8545;D Loss: -1.4471; GP: 0.2744\n",
      "now, we train G 2 times with (prob fake = -0.854, prob real = 0.623)\n",
      "Mean x/edge attr:  tensor([0.7425, 0.1230, 0.0895, 0.0321, 0.0089, 0.0041, 0.0000, 0.0000, 0.0000,\n",
      "        0.9898, 0.0102, 0.0000, 0.9611, 0.0225, 0.0164], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7715, 0.2148, 0.0137], device='cuda:2', grad_fn=<MeanBackward1>) tensor(50.1250, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 22.875 tensor([19., 21., 19., 19., 29., 29., 23., 25., 18., 29., 29., 22., 19., 29.,\n",
      "        17., 25., 27., 25., 18., 18., 18., 31., 22., 16., 29., 29., 17., 27.,\n",
      "        17., 18., 27., 27., 16., 18., 20., 29., 18., 13., 15., 29., 26., 18.,\n",
      "        31., 18., 29., 29., 16., 21., 27., 23., 25., 29., 27., 21., 31., 31.,\n",
      "        16., 27., 13., 19., 25., 16., 25., 25.], device='cuda:2')\n",
      "Sample prob: -26.99923324584961\n",
      "[1/200][2400/3892]\tG Loss: 0.7600;D Loss: -1.4234; GP: 0.2482\n",
      "now, we train G 2 times with (prob fake = -0.760, prob real = 0.662)\n",
      "Mean x/edge attr:  tensor([0.7003, 0.1394, 0.1104, 0.0135, 0.0236, 0.0061, 0.0067, 0.0000, 0.0000,\n",
      "        0.9879, 0.0121, 0.0000, 0.9778, 0.0162, 0.0061], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7410, 0.2485, 0.0105], device='cuda:2', grad_fn=<MeanBackward1>) tensor(50.4375, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.203125 tensor([17., 12., 15., 19., 25., 29., 27., 27., 13., 31., 31., 16., 17., 13.,\n",
      "        23., 23., 31., 27., 29., 31., 16., 27., 17., 14., 25., 25., 25., 29.,\n",
      "        31., 27., 33., 27., 16., 31., 18., 14., 31., 27., 18., 14., 26., 13.,\n",
      "        29., 18., 16., 29., 18., 26., 31., 33., 27., 14., 27., 27., 22., 29.,\n",
      "        29., 31., 27., 15., 31., 16., 17., 13.], device='cuda:2')\n",
      "Sample prob: -26.309520721435547\n",
      "Validation, uniqueness, novelty:  (0.253, 1.0, 1.0)\n",
      "[1/200][2500/3892]\tG Loss: 0.8304;D Loss: -1.5491; GP: 0.3169\n",
      "now, we train G 2 times with (prob fake = -0.830, prob real = 0.733)\n",
      "Mean x/edge attr:  tensor([0.7469, 0.0926, 0.1204, 0.0029, 0.0131, 0.0241, 0.0000, 0.0000, 0.0000,\n",
      "        0.9774, 0.0182, 0.0044, 0.9621, 0.0190, 0.0190], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7505, 0.2495, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(45.8438, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 21.421875 tensor([23., 17., 16., 17., 17., 27., 25., 15., 18., 21., 17., 29., 23., 17.,\n",
      "        25., 25., 25., 17., 16., 20., 22., 14., 15., 28., 27., 27., 29., 20.,\n",
      "        23., 21., 25., 27., 16., 25., 18., 21., 26., 27., 14., 25., 25., 18.,\n",
      "        16., 19., 16., 24., 17., 16., 27., 22., 23., 27., 17., 26., 17., 23.,\n",
      "        23., 29., 14., 25., 23., 15., 22., 27.], device='cuda:2')\n",
      "Sample prob: -24.30221176147461\n",
      "[1/200][2600/3892]\tG Loss: 0.6197;D Loss: -1.5410; GP: 0.1632\n",
      "now, we train G 2 times with (prob fake = -0.620, prob real = 0.875)\n",
      "Mean x/edge attr:  tensor([0.7424, 0.1155, 0.0939, 0.0254, 0.0209, 0.0019, 0.0000, 0.0000, 0.0000,\n",
      "        0.9791, 0.0140, 0.0070, 0.9651, 0.0178, 0.0171], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7286, 0.2714, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(53.3125, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 24.625 tensor([21., 18., 25., 21., 33., 22., 33., 21., 18., 18., 27., 19., 31., 22.,\n",
      "        29., 21., 31., 29., 18., 20., 20., 31., 31., 31., 27., 28., 31., 18.,\n",
      "        17., 16., 15., 31., 31., 20., 19., 31., 19., 18., 18., 29., 33., 31.,\n",
      "        17., 32., 18., 18., 23., 31., 21., 33., 24., 23., 21., 33., 19., 23.,\n",
      "        27., 31., 20., 27., 31., 20., 29., 33.], device='cuda:2')\n",
      "Sample prob: -27.710689544677734\n",
      "Validation, uniqueness, novelty:  (0.231, 1.0, 1.0)\n",
      "[1/200][2700/3892]\tG Loss: 0.6628;D Loss: -1.5352; GP: 0.5027\n",
      "now, we train G 2 times with (prob fake = -0.663, prob real = 0.835)\n",
      "Mean x/edge attr:  tensor([0.7515, 0.1222, 0.1047, 0.0020, 0.0155, 0.0041, 0.0000, 0.0000, 0.0000,\n",
      "        0.9797, 0.0176, 0.0027, 0.9656, 0.0155, 0.0189], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7430, 0.2538, 0.0032], device='cuda:2', grad_fn=<MeanBackward1>) tensor(49.3750, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.140625 tensor([18., 17., 14., 25., 27., 25., 31., 25., 27., 15., 25., 14., 23., 19.,\n",
      "        17., 18., 27., 27., 31., 29., 16., 26., 27., 29., 25., 25., 16., 18.,\n",
      "        18., 25., 27., 16., 29., 29., 29., 27., 25., 19., 17., 27., 31., 31.,\n",
      "        21., 23., 15., 29., 14., 27., 27., 23., 29., 23., 17., 29., 20., 21.,\n",
      "        20., 16., 29., 29., 27., 17., 19., 20.], device='cuda:2')\n",
      "Sample prob: -26.10192108154297\n",
      "[1/200][2800/3892]\tG Loss: 0.8327;D Loss: -1.5395; GP: 0.3416\n",
      "now, we train G 2 times with (prob fake = -0.833, prob real = 0.708)\n",
      "Mean x/edge attr:  tensor([0.7444, 0.1137, 0.1010, 0.0177, 0.0078, 0.0071, 0.0085, 0.0000, 0.0000,\n",
      "        0.9788, 0.0212, 0.0000, 0.9407, 0.0339, 0.0254], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7715, 0.2236, 0.0049], device='cuda:2', grad_fn=<MeanBackward1>) tensor(47.6562, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 22.125 tensor([13., 21., 13., 19., 12., 23., 27., 31., 27., 17., 25., 15., 18., 17.,\n",
      "        17., 28., 18., 29., 15., 27., 18., 25., 27., 27., 22., 17., 18., 29.,\n",
      "        15., 29., 27., 16., 13., 29., 27., 16., 23., 15., 27., 31., 23., 25.,\n",
      "        29., 24., 31., 29., 27., 27., 18., 26., 27., 25., 16., 27., 16., 16.,\n",
      "        19., 25., 19., 17., 27., 31., 14., 15.], device='cuda:2')\n",
      "Sample prob: -24.52120590209961\n",
      "Validation, uniqueness, novelty:  (0.456, 1.0, 1.0)\n",
      "[1/200][2900/3892]\tG Loss: 0.8275;D Loss: -1.5614; GP: 0.4054\n",
      "now, we train G 2 times with (prob fake = -0.828, prob real = 0.730)\n",
      "Mean x/edge attr:  tensor([7.5140e-01, 1.1134e-01, 1.0224e-01, 1.1905e-02, 7.0028e-03, 1.5406e-02,\n",
      "        7.0028e-04, 0.0000e+00, 0.0000e+00, 9.8249e-01, 1.1905e-02, 5.6022e-03,\n",
      "        9.5518e-01, 3.5014e-02, 9.8039e-03], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7606, 0.2368, 0.0026], device='cuda:2', grad_fn=<MeanBackward1>) tensor(48.0312, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 22.3125 tensor([21., 20., 25., 29., 22., 19., 21., 19., 20., 25., 21., 29., 27., 25.,\n",
      "        29., 23., 21., 27., 33., 23., 24., 21., 25., 19., 27., 19., 18., 21.,\n",
      "        19., 21., 16., 27., 17., 23., 23., 18., 26., 18., 20., 21., 25., 25.,\n",
      "        21., 25., 27., 27., 20., 18., 15., 21., 20., 27., 17., 17., 20., 18.,\n",
      "        21., 23., 25., 27., 19., 20., 25., 23.], device='cuda:2')\n",
      "Sample prob: -23.45907211303711\n",
      "[1/200][3000/3892]\tG Loss: 0.9070;D Loss: -1.5118; GP: 0.5899\n",
      "now, we train G 2 times with (prob fake = -0.907, prob real = 0.569)\n",
      "Mean x/edge attr:  tensor([7.7592e-01, 9.5338e-02, 1.0926e-01, 6.2630e-03, 2.7836e-03, 9.0466e-03,\n",
      "        6.9589e-04, 6.9589e-04, 0.0000e+00, 9.7982e-01, 1.8093e-02, 2.0877e-03,\n",
      "        9.2693e-01, 3.3403e-02, 3.9666e-02], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7951, 0.2040, 0.0010], device='cuda:2', grad_fn=<MeanBackward1>) tensor(48.1875, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 22.453125 tensor([25., 23., 22., 28., 29., 21., 24., 25., 25., 23., 22., 21., 21., 23.,\n",
      "        25., 23., 29., 21., 19., 20., 20., 23., 20., 21., 23., 22., 19., 25.,\n",
      "        27., 23., 21., 21., 20., 23., 21., 21., 19., 23., 21., 25., 19., 22.,\n",
      "        18., 21., 24., 19., 25., 24., 19., 19., 23., 27., 18., 25., 23., 22.,\n",
      "        29., 22., 22., 27., 20., 20., 21., 21.], device='cuda:2')\n",
      "Sample prob: -20.97591781616211\n",
      "Validation, uniqueness, novelty:  (0.373, 1.0, 1.0)\n",
      "[1/200][3100/3892]\tG Loss: 0.9653;D Loss: -1.5687; GP: 0.4570\n",
      "now, we train G 2 times with (prob fake = -0.965, prob real = 0.570)\n",
      "Mean x/edge attr:  tensor([0.7227, 0.1210, 0.1004, 0.0270, 0.0180, 0.0109, 0.0000, 0.0000, 0.0000,\n",
      "        0.9807, 0.0167, 0.0026, 0.9653, 0.0257, 0.0090], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7114, 0.2838, 0.0048], device='cuda:2', grad_fn=<MeanBackward1>) tensor(51.8125, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 24.28125 tensor([25., 23., 23., 22., 22., 25., 23., 25., 21., 23., 26., 27., 27., 22.,\n",
      "        27., 25., 21., 23., 25., 27., 23., 27., 20., 21., 22., 25., 33., 25.,\n",
      "        23., 22., 26., 31., 23., 25., 31., 25., 29., 19., 29., 25., 25., 23.,\n",
      "        25., 21., 25., 21., 23., 25., 24., 23., 20., 24., 29., 25., 27., 23.,\n",
      "        25., 30., 22., 18., 21., 23., 21., 25.], device='cuda:2')\n",
      "Sample prob: -23.372650146484375\n",
      "[1/200][3200/3892]\tG Loss: 0.8479;D Loss: -1.4944; GP: 0.4611\n",
      "now, we train G 2 times with (prob fake = -0.848, prob real = 0.620)\n",
      "Mean x/edge attr:  tensor([0.7511, 0.1196, 0.1048, 0.0071, 0.0109, 0.0051, 0.0013, 0.0000, 0.0000,\n",
      "        0.9736, 0.0244, 0.0019, 0.9511, 0.0090, 0.0399], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7317, 0.2683, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(52.4688, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 24.296875 tensor([22., 29., 21., 31., 23., 21., 24., 23., 27., 19., 20., 27., 27., 30.,\n",
      "        29., 24., 25., 22., 26., 21., 27., 21., 23., 27., 25., 25., 22., 25.,\n",
      "        29., 20., 25., 27., 21., 26., 23., 27., 31., 20., 20., 19., 21., 25.,\n",
      "        27., 31., 23., 21., 25., 20., 25., 29., 27., 29., 21., 25., 29., 19.,\n",
      "        19., 27., 23., 23., 27., 16., 29., 20.], device='cuda:2')\n",
      "Sample prob: -24.50961685180664\n",
      "Validation, uniqueness, novelty:  (0.173, 1.0, 1.0)\n",
      "[1/200][3300/3892]\tG Loss: 0.7375;D Loss: -1.6545; GP: 0.3332\n",
      "now, we train G 2 times with (prob fake = -0.738, prob real = 0.852)\n",
      "Mean x/edge attr:  tensor([0.7509, 0.1239, 0.0945, 0.0056, 0.0168, 0.0042, 0.0042, 0.0000, 0.0000,\n",
      "        0.9783, 0.0217, 0.0000, 0.9496, 0.0070, 0.0434], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7568, 0.2432, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(48.0625, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 22.328125 tensor([17., 23., 18., 25., 15., 22., 24., 22., 25., 25., 20., 31., 16., 29.,\n",
      "        16., 21., 29., 18., 23., 15., 25., 21., 17., 25., 16., 23., 25., 31.,\n",
      "        18., 23., 19., 21., 18., 25., 19., 27., 27., 21., 24., 23., 25., 31.,\n",
      "        27., 19., 27., 15., 21., 31., 25., 27., 21., 25., 15., 19., 25., 25.,\n",
      "        13., 17., 19., 27., 16., 27., 25., 25.], device='cuda:2')\n",
      "Sample prob: -20.73411750793457\n",
      "[1/200][3400/3892]\tG Loss: 0.8186;D Loss: -1.5925; GP: 0.2527\n",
      "now, we train G 2 times with (prob fake = -0.819, prob real = 0.755)\n",
      "Mean x/edge attr:  tensor([7.2771e-01, 1.2440e-01, 9.7443e-02, 2.3497e-02, 2.3497e-02, 2.7643e-03,\n",
      "        6.9109e-04, 0.0000e+00, 0.0000e+00, 9.7719e-01, 1.7277e-02, 5.5287e-03,\n",
      "        9.6545e-01, 2.9717e-02, 4.8376e-03], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7065, 0.2935, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(48.6562, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 22.609375 tensor([15., 18., 18., 27., 17., 25., 20., 21., 27., 25., 25., 19., 17., 25.,\n",
      "        27., 27., 29., 15., 32., 19., 18., 19., 25., 23., 27., 23., 18., 22.,\n",
      "        27., 27., 18., 19., 29., 18., 29., 29., 27., 17., 19., 19., 17., 18.,\n",
      "        25., 19., 19., 27., 18., 19., 27., 31., 27., 17., 27., 29., 15., 25.,\n",
      "        19., 29., 27., 19., 17., 27., 31., 17.], device='cuda:2')\n",
      "Sample prob: -20.72164535522461\n",
      "Validation, uniqueness, novelty:  (0.358, 1.0, 1.0)\n",
      "[1/200][3500/3892]\tG Loss: 0.7454;D Loss: -1.5269; GP: 0.2554\n",
      "now, we train G 2 times with (prob fake = -0.745, prob real = 0.780)\n",
      "Mean x/edge attr:  tensor([7.0839e-01, 1.2844e-01, 9.5020e-02, 4.9148e-02, 1.8349e-02, 6.5531e-04,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 9.7969e-01, 1.9659e-02, 6.5531e-04,\n",
      "        9.6592e-01, 1.2451e-02, 2.1625e-02], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7435, 0.2565, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(51.3438, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.84375 tensor([18., 30., 18., 20., 16., 29., 31., 31., 17., 29., 19., 29., 29., 31.,\n",
      "        33., 19., 29., 29., 20., 29., 33., 19., 18., 17., 27., 21., 19., 14.,\n",
      "        29., 15., 27., 20., 19., 31., 16., 18., 31., 33., 29., 25., 21., 31.,\n",
      "        18., 29., 29., 25., 27., 17., 19., 15., 29., 22., 31., 31., 27., 17.,\n",
      "        22., 12., 18., 31., 22., 31., 17., 18.], device='cuda:2')\n",
      "Sample prob: -22.203960418701172\n",
      "[1/200][3600/3892]\tG Loss: 0.7953;D Loss: -1.6848; GP: 0.6803\n",
      "now, we train G 2 times with (prob fake = -0.795, prob real = 0.850)\n",
      "Mean x/edge attr:  tensor([7.3913e-01, 1.2675e-01, 1.0391e-01, 5.1584e-03, 1.4738e-02, 7.3692e-03,\n",
      "        2.2108e-03, 0.0000e+00, 7.3692e-04, 9.6021e-01, 3.9794e-02, 0.0000e+00,\n",
      "        9.4326e-01, 4.3478e-02, 1.3265e-02], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7825, 0.2175, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(45.9062, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 21.203125 tensor([14., 13., 23., 27., 27., 15., 15., 20., 23., 27., 18., 25., 18., 25.,\n",
      "        25., 15., 25., 21., 29., 16., 23., 25., 15., 18., 27., 25., 24., 17.,\n",
      "        21., 25., 15., 22., 21., 27., 18., 27., 29., 18., 25., 25., 15., 15.,\n",
      "        25., 15., 21., 21., 25., 20., 27., 13., 21., 15., 21., 25., 25., 21.,\n",
      "        15., 20., 29., 23., 20., 21., 21., 15.], device='cuda:2')\n",
      "Sample prob: -19.239791870117188\n",
      "Validation, uniqueness, novelty:  (0.406, 1.0, 1.0)\n",
      "[1/200][3700/3892]\tG Loss: 0.6280;D Loss: -1.5527; GP: 0.3563\n",
      "now, we train G 2 times with (prob fake = -0.628, prob real = 0.947)\n",
      "Mean x/edge attr:  tensor([7.3171e-01, 1.3579e-01, 9.8220e-02, 1.1206e-02, 1.0547e-02, 1.0547e-02,\n",
      "        1.3184e-03, 6.5920e-04, 0.0000e+00, 9.7165e-01, 2.2413e-02, 5.9328e-03,\n",
      "        9.5979e-01, 1.7139e-02, 2.3072e-02], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7388, 0.2612, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(50.7812, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.703125 tensor([25., 31., 23., 18., 27., 29., 27., 17., 27., 17., 20., 11., 23., 28.,\n",
      "        31., 14., 18., 17., 26., 29., 14., 18., 29., 25., 14., 12., 25., 25.,\n",
      "        25., 15., 27., 11., 11., 27., 29., 27., 31., 17., 27., 31., 29., 27.,\n",
      "        27., 29., 27., 29., 25., 25., 15., 29., 23., 15., 29., 27., 29., 29.,\n",
      "        29., 30., 29., 23., 27., 25., 17., 25.], device='cuda:2')\n",
      "Sample prob: -22.990577697753906\n",
      "[1/200][3800/3892]\tG Loss: 0.6534;D Loss: -1.4686; GP: 0.2328\n",
      "now, we train G 2 times with (prob fake = -0.653, prob real = 0.819)\n",
      "Mean x/edge attr:  tensor([0.7248, 0.1470, 0.0882, 0.0134, 0.0267, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.9820, 0.0180, 0.0000, 0.9666, 0.0187, 0.0147], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7486, 0.2514, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(49.6562, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.390625 tensor([25., 21., 23., 23., 27., 21., 26., 19., 25., 20., 21., 21., 19., 27.,\n",
      "        21., 20., 21., 27., 23., 23., 29., 19., 23., 23., 23., 24., 23., 19.,\n",
      "        19., 23., 27., 25., 25., 29., 22., 25., 25., 23., 27., 25., 23., 21.,\n",
      "        19., 29., 21., 27., 21., 21., 29., 19., 23., 25., 23., 25., 21., 27.,\n",
      "        25., 23., 27., 21., 23., 23., 23., 27.], device='cuda:2')\n",
      "Sample prob: -21.052860260009766\n",
      "Validation, uniqueness, novelty:  (0.148, 1.0, 1.0)\n",
      "less than 64\n",
      "[2/200][9/3892]\tG Loss: 0.8805;D Loss: -1.5595; GP: 0.2796\n",
      "now, we train G 2 times with (prob fake = -0.881, prob real = 0.674)\n",
      "Mean x/edge attr:  tensor([7.6282e-01, 1.2192e-01, 9.0606e-02, 3.9973e-03, 1.6656e-02, 3.3311e-03,\n",
      "        6.6622e-04, 0.0000e+00, 0.0000e+00, 9.8135e-01, 1.1326e-02, 7.3284e-03,\n",
      "        9.5470e-01, 1.5989e-02, 2.9314e-02], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7402, 0.2598, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(50.0938, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.453125 tensor([25., 21., 23., 25., 19., 22., 25., 21., 25., 23., 23., 23., 23., 21.,\n",
      "        25., 25., 21., 21., 27., 20., 23., 19., 23., 22., 25., 23., 23., 21.,\n",
      "        25., 27., 23., 27., 23., 23., 21., 25., 23., 23., 27., 23., 26., 25.,\n",
      "        27., 23., 23., 21., 25., 23., 21., 25., 23., 23., 23., 25., 25., 23.,\n",
      "        25., 21., 27., 24., 27., 25., 24., 20.], device='cuda:2')\n",
      "Sample prob: -19.937562942504883\n",
      "[2/200][109/3892]\tG Loss: 0.7671;D Loss: -1.6546; GP: 0.2991\n",
      "now, we train G 2 times with (prob fake = -0.767, prob real = 0.865)\n",
      "Mean x/edge attr:  tensor([0.7291, 0.1249, 0.0898, 0.0077, 0.0295, 0.0091, 0.0098, 0.0000, 0.0000,\n",
      "        0.9888, 0.0049, 0.0063, 0.9740, 0.0175, 0.0084], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7441, 0.2559, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(46.2188, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 22.265625 tensor([19., 25., 21., 23., 23., 21., 21., 21., 25., 21., 21., 25., 21., 23.,\n",
      "        23., 22., 19., 23., 21., 23., 21., 22., 23., 25., 25., 21., 23., 23.,\n",
      "        24., 20., 21., 23., 21., 23., 23., 25., 21., 21., 19., 25., 25., 21.,\n",
      "        23., 23., 23., 21., 19., 25., 19., 23., 19., 23., 23., 21., 21., 23.,\n",
      "        24., 21., 23., 25., 23., 20., 24., 23.], device='cuda:2')\n",
      "Sample prob: -18.77919578552246\n",
      "Validation, uniqueness, novelty:  (0.258, 1.0, 1.0)\n",
      "[2/200][209/3892]\tG Loss: 0.9606;D Loss: -1.5871; GP: 0.2357\n",
      "now, we train G 2 times with (prob fake = -0.961, prob real = 0.654)\n",
      "Mean x/edge attr:  tensor([0.7394, 0.1194, 0.0989, 0.0165, 0.0178, 0.0059, 0.0020, 0.0000, 0.0000,\n",
      "        0.9888, 0.0059, 0.0053, 0.9690, 0.0310, 0.0000], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7312, 0.2688, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(50.7500, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.6875 tensor([23., 29., 23., 27., 21., 25., 23., 24., 21., 21., 25., 25., 23., 21.,\n",
      "        23., 23., 29., 25., 23., 25., 25., 24., 21., 21., 24., 23., 27., 26.,\n",
      "        27., 25., 25., 25., 23., 21., 23., 25., 23., 21., 23., 23., 19., 23.,\n",
      "        23., 21., 21., 23., 23., 23., 21., 25., 25., 21., 29., 25., 23., 23.,\n",
      "        25., 23., 25., 23., 27., 25., 23., 23.], device='cuda:2')\n",
      "Sample prob: -20.5454044342041\n",
      "[2/200][309/3892]\tG Loss: 1.0608;D Loss: -1.6127; GP: 0.1956\n",
      "now, we train G 2 times with (prob fake = -1.061, prob real = 0.610)\n",
      "Mean x/edge attr:  tensor([0.7592, 0.1119, 0.1023, 0.0089, 0.0089, 0.0055, 0.0034, 0.0000, 0.0000,\n",
      "        0.9911, 0.0089, 0.0000, 0.9420, 0.0109, 0.0471], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7381, 0.2619, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(49.1562, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 22.90625 tensor([23., 23., 22., 25., 23., 25., 21., 22., 23., 19., 21., 23., 21., 23.,\n",
      "        23., 21., 23., 23., 23., 21., 23., 25., 21., 23., 21., 25., 21., 27.,\n",
      "        22., 23., 23., 27., 19., 22., 25., 25., 23., 23., 23., 21., 24., 23.,\n",
      "        23., 23., 23., 21., 25., 27., 25., 21., 21., 23., 21., 23., 23., 21.,\n",
      "        23., 23., 23., 29., 23., 21., 23., 24.], device='cuda:2')\n",
      "Sample prob: -20.365684509277344\n",
      "Validation, uniqueness, novelty:  (0.107, 1.0, 1.0)\n",
      "[2/200][409/3892]\tG Loss: 0.9198;D Loss: -1.5376; GP: 0.2895\n",
      "now, we train G 2 times with (prob fake = -0.920, prob real = 0.641)\n",
      "Mean x/edge attr:  tensor([0.7498, 0.1159, 0.0875, 0.0081, 0.0319, 0.0047, 0.0020, 0.0000, 0.0000,\n",
      "        0.9722, 0.0197, 0.0081, 0.9607, 0.0061, 0.0332], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7307, 0.2693, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(49.4375, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.046875 tensor([22., 27., 25., 21., 21., 26., 21., 21., 23., 25., 25., 23., 23., 23.,\n",
      "        21., 23., 22., 21., 21., 21., 23., 25., 23., 25., 25., 23., 23., 22.,\n",
      "        21., 23., 23., 23., 23., 25., 23., 21., 27., 21., 25., 21., 23., 26.,\n",
      "        25., 23., 23., 21., 21., 25., 19., 23., 24., 24., 27., 21., 21., 21.,\n",
      "        27., 23., 23., 23., 23., 23., 25., 21.], device='cuda:2')\n",
      "Sample prob: -18.78573226928711\n",
      "[2/200][509/3892]\tG Loss: 1.1276;D Loss: -1.6099; GP: 0.2931\n",
      "now, we train G 2 times with (prob fake = -1.128, prob real = 0.478)\n",
      "Mean x/edge attr:  tensor([0.7291, 0.1061, 0.1404, 0.0020, 0.0165, 0.0059, 0.0000, 0.0000, 0.0000,\n",
      "        0.9651, 0.0270, 0.0079, 0.9604, 0.0231, 0.0165], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7486, 0.2514, 0.0000], device='cuda:2', grad_fn=<MeanBackward1>) tensor(51.5312, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.703125 tensor([21., 25., 25., 25., 21., 23., 23., 29., 21., 21., 21., 25., 25., 27.,\n",
      "        21., 25., 23., 23., 25., 23., 21., 23., 25., 27., 23., 25., 25., 23.,\n",
      "        23., 23., 23., 23., 23., 25., 23., 23., 23., 25., 27., 29., 21., 27.,\n",
      "        23., 25., 28., 27., 23., 25., 21., 23., 23., 21., 21., 23., 25., 25.,\n",
      "        25., 25., 21., 25., 23., 23., 21., 19.], device='cuda:2')\n",
      "Sample prob: -19.932910919189453\n",
      "Validation, uniqueness, novelty:  (0.04, 1.0, 1.0)\n",
      "[2/200][609/3892]\tG Loss: 0.9057;D Loss: -1.6138; GP: 0.1955\n",
      "now, we train G 2 times with (prob fake = -0.906, prob real = 0.708)\n",
      "Mean x/edge attr:  tensor([0.7200, 0.1400, 0.0961, 0.0041, 0.0144, 0.0254, 0.0000, 0.0000, 0.0000,\n",
      "        0.9835, 0.0055, 0.0110, 0.9664, 0.0075, 0.0261], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7305, 0.2589, 0.0105], device='cuda:2', grad_fn=<MeanBackward1>) tensor(49., device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 22.765625 tensor([27., 23., 21., 24., 25., 21., 25., 25., 21., 19., 21., 23., 23., 25.,\n",
      "        23., 21., 21., 25., 23., 27., 23., 21., 23., 25., 21., 27., 23., 25.,\n",
      "        23., 21., 25., 23., 23., 23., 23., 26., 23., 21., 23., 23., 21., 23.,\n",
      "        23., 21., 23., 21., 20., 21., 21., 25., 25., 21., 23., 23., 25., 21.,\n",
      "        23., 21., 21., 21., 19., 23., 21., 23.], device='cuda:2')\n",
      "Sample prob: -18.58603286743164\n",
      "[2/200][709/3892]\tG Loss: 1.1129;D Loss: -1.5450; GP: 0.2530\n",
      "now, we train G 2 times with (prob fake = -1.113, prob real = 0.439)\n",
      "Mean x/edge attr:  tensor([0.7396, 0.1177, 0.1190, 0.0020, 0.0105, 0.0053, 0.0059, 0.0000, 0.0000,\n",
      "        0.9645, 0.0335, 0.0020, 0.9448, 0.0072, 0.0480], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7565, 0.2410, 0.0024], device='cuda:2', grad_fn=<MeanBackward1>) tensor(51.3438, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.765625 tensor([21., 21., 25., 23., 21., 23., 25., 25., 25., 23., 27., 25., 25., 23.,\n",
      "        23., 23., 25., 23., 27., 25., 23., 21., 21., 23., 25., 24., 21., 23.,\n",
      "        27., 23., 23., 25., 27., 25., 27., 21., 27., 21., 21., 25., 25., 19.,\n",
      "        25., 25., 23., 23., 25., 27., 25., 23., 29., 23., 21., 23., 21., 25.,\n",
      "        23., 23., 25., 21., 25., 25., 21., 25.], device='cuda:2')\n",
      "Sample prob: -18.132217407226562\n",
      "Validation, uniqueness, novelty:  (0.144, 1.0, 1.0)\n",
      "[2/200][809/3892]\tG Loss: 0.9881;D Loss: -1.5662; GP: 0.2970\n",
      "now, we train G 2 times with (prob fake = -0.988, prob real = 0.604)\n",
      "Mean x/edge attr:  tensor([7.3904e-01, 1.2542e-01, 1.1194e-01, 1.0115e-02, 1.0115e-02, 3.3715e-03,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 9.8449e-01, 1.4835e-02, 6.7431e-04,\n",
      "        9.6022e-01, 3.1693e-02, 8.0917e-03], device='cuda:2',\n",
      "       grad_fn=<MeanBackward1>) tensor([0.7537, 0.2360, 0.0103], device='cuda:2', grad_fn=<MeanBackward1>) tensor(50.1250, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "size x/ some distribution: 23.171875 tensor([23., 23., 23., 23., 23., 25., 21., 27., 23., 19., 24., 23., 23., 25.,\n",
      "        25., 23., 25., 25., 21., 25., 20., 23., 21., 23., 21., 23., 25., 25.,\n",
      "        27., 23., 23., 25., 25., 21., 23., 21., 24., 25., 23., 23., 25., 23.,\n",
      "        23., 23., 21., 21., 20., 21., 21., 23., 21., 25., 21., 22., 25., 21.,\n",
      "        23., 23., 27., 21., 23., 25., 27., 25.], device='cuda:2')\n",
      "Sample prob: -17.978174209594727\n"
     ]
    }
   ],
   "source": [
    "train.train(data_loader, verbose=False, use_data_x = 15, use_data_edgeattr=3, \\\n",
    "        evaluate_num=1000, mol_data=mol_data, alter_trainer=True, NN=200, \\\n",
    "            reinforce_acclerate=True\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "candidate_evals = [(i,j ) for i,j in enumerate(train.evals) if j[0] > 0.9]\n",
    "candidate_metrics = [np.prod(j) for i,j in enumerate(train.evals) if j[0] > 0.9]\n",
    "choose_model = candidate_evals[np.argmax(candidate_metrics)][0]\n",
    "generator = torch.load(os.path.join(train.folder, f'generator_{choose_model}.pt'))\n",
    "torch.save(generator, 'ul_gan_zinc.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
